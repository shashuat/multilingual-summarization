{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Data/shash/.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in to Hugging Face successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /users/eleves-b/2024/shashwat.sharma/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzshashwatz\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in to Weights & Biases successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/Data/shash/.cache\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/Data/shash/.cache/hub\"\n",
    "\n",
    "print(os.environ[\"HF_HOME\"])\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "wandb_token = os.getenv(\"WANDB_TOKEN\")\n",
    "\n",
    "if hf_token:\n",
    "    login(hf_token)\n",
    "    print(\"Logged in to Hugging Face successfully!\")\n",
    "else:\n",
    "    print(\"HF_TOKEN not found in .env file.\")\n",
    "\n",
    "if wandb_token:\n",
    "    wandb.login(key=wandb_token)\n",
    "    print(\"Logged in to Weights & Biases successfully!\")\n",
    "else:\n",
    "    print(\"WANDB_TOKEN not found in .env file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phi-4 Fine-tuning Debug Notebook\n",
    "# ===========================\n",
    "# This notebook will help diagnose issues with the fine-tuned model for multilingual summarization\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from rouge import Rouge\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define paths and parameters\n",
    "BASE_MODEL = \"microsoft/Phi-4-mini-instruct\"\n",
    "FINETUNED_MODEL = \"/Data/shash/mul/finetuned_models/checkpoint-1497\"\n",
    "DATASET_PATH = \"/Data/shash/mul/hf_dataset\"\n",
    "LANGUAGE = \"fr\"  # French\n",
    "MAX_INPUT_LENGTH = 3072\n",
    "MAX_OUTPUT_LENGTH = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: microsoft/Phi-4-mini-instruct\n",
      "Fine-tuned model: /Data/shash/mul/finetuned_models/checkpoint-1497\n",
      "Dataset path: /Data/shash/mul/hf_dataset/fr\n",
      "\n",
      "=== Loading Dataset ===\n",
      "Loaded 500 test examples\n",
      "\n",
      "Sample example (index 0):\n",
      "Title: Nihonium\n",
      "Article length: 7829 chars\n",
      "Reference summary length: 749 chars\n",
      "\n",
      "First 300 chars of article text:\n",
      "{{confusion|Technétium#À la recherche de l'élément chimique 43{{!}}Nipponium}}\n",
      "{{Infobox Élément/Nihonium}}\n",
      "Le '''nihonium''' ([[Symbole chimique|symbole]] '''Nh''') est l'[[élément chimique]] de [[numéro atomique]] 113. Il correspond à l'ununtrium (Uut) de la [[dénomination systématique]] de l'[[Un...\n",
      "\n",
      "Reference summary:\n",
      "Le nihonium (symbole Nh) est un élément chimique de numéro atomique 113, initialement appelé ununtrium. Il a été synthétisé pour la première fois en juillet 2004 au RIKEN près de Tokyo, et son identification a été validée par l'IUPAC en décembre 2015. Le nihonium est un transactinide radioactif, avec l'isotope le plus stable, le 286Nh, ayant une période radioactive de 19,6 secondes. Il est situé sous le thallium dans le tableau périodique et pourrait avoir des propriétés chimiques similaires à celles d'un métal pauvre. L'IUPAC a officiellement nommé l'élément en novembre 2016 en référence au Japon. L'élément a été initialement découvert par une équipe russo-américaine en août 2003, mais la synthèse japonaise a été confirmée comme première.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Base model: {BASE_MODEL}\")\n",
    "print(f\"Fine-tuned model: {FINETUNED_MODEL}\")\n",
    "print(f\"Dataset path: {DATASET_PATH}/{LANGUAGE}\")\n",
    "\n",
    "# 1. Load the dataset\n",
    "print(\"\\n=== Loading Dataset ===\")\n",
    "dataset = load_from_disk(os.path.join(DATASET_PATH, LANGUAGE))\n",
    "test_dataset = dataset[\"test\"]\n",
    "print(f\"Loaded {len(test_dataset)} test examples\")\n",
    "\n",
    "# Display a sample\n",
    "sample_idx = 0\n",
    "sample = test_dataset[sample_idx]\n",
    "print(f\"\\nSample example (index {sample_idx}):\")\n",
    "print(f\"Title: {sample['title']}\")\n",
    "print(f\"Article length: {len(sample['text'])} chars\")\n",
    "print(f\"Reference summary length: {len(sample['summary'])} chars\")\n",
    "print(f\"\\nFirst 300 chars of article text:\")\n",
    "print(sample['text'][:300] + \"...\")\n",
    "print(f\"\\nReference summary:\")\n",
    "print(sample['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Base Model ===\n",
      "Tokenizer vocab size: 200029\n",
      "Special tokens: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d57e9d086f41f98b095a73a4faaac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded: Phi3ForCausalLM\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Load base model and tokenizer\n",
    "print(\"\\n=== Loading Base Model ===\")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# Ensure padding token is set correctly\n",
    "if base_tokenizer.pad_token is None:\n",
    "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "    print(\"Set pad_token to eos_token\")\n",
    "\n",
    "print(f\"Tokenizer vocab size: {len(base_tokenizer)}\")\n",
    "print(f\"Special tokens: {base_tokenizer.special_tokens_map}\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(f\"Base model loaded: {type(base_model).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Fine-tuned Model ===\n",
      "Found adapter_config.json - loading as PEFT/LoRA model\n",
      "PEFT config: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='microsoft/Phi-4-mini-instruct', revision=None, inference_mode=True, r=16, target_modules={'Wqkv', 'down_proj', 'up_proj', 'out_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n",
      "Successfully loaded as PeftModel\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Load fine-tuned model\n",
    "print(\"\\n=== Loading Fine-tuned Model ===\")\n",
    "\n",
    "# Check if adapter config exists (PEFT/LoRA)\n",
    "adapter_config_path = os.path.join(FINETUNED_MODEL, \"adapter_config.json\")\n",
    "is_peft_model = os.path.exists(adapter_config_path)\n",
    "\n",
    "if is_peft_model:\n",
    "    print(f\"Found adapter_config.json - loading as PEFT/LoRA model\")\n",
    "    try:\n",
    "        # Load PEFT configuration\n",
    "        peft_config = PeftConfig.from_pretrained(FINETUNED_MODEL)\n",
    "        print(f\"PEFT config: {peft_config}\")\n",
    "        \n",
    "        # Load as PeftModel (adapter on top of base model)\n",
    "        finetuned_model = PeftModel.from_pretrained(\n",
    "            base_model, \n",
    "            FINETUNED_MODEL\n",
    "        )\n",
    "        print(\"Successfully loaded as PeftModel\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading as PEFT model: {e}\")\n",
    "        is_peft_model = False\n",
    "        \n",
    "if not is_peft_model:\n",
    "    print(\"Loading as standard model (not PEFT/LoRA)\")\n",
    "    try:\n",
    "        finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "            FINETUNED_MODEL,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        print(\"Successfully loaded as standard model\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 4. Helper functions for prompt creation and generation\n",
    "def get_system_message(language=\"fr\"):\n",
    "    \"\"\"Get the appropriate system message based on language\"\"\"\n",
    "    if language == \"fr\":\n",
    "        return \"\"\"Vous êtes un expert en résumé. Votre tâche est de créer des résumés concis et complets \n",
    "qui capturent tous les points clés et les arguments principaux tout en évitant les détails inutiles. \n",
    "Ne copiez pas simplement les premières phrases de l'article. Créez un résumé qui se tient \n",
    "par lui-même et couvre tout le contenu important de l'article.\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"You are an expert summarizer. Your task is to create concise, comprehensive summaries \n",
    "in {language} that capture all key points and main arguments while avoiding unnecessary details. \n",
    "Do not simply copy the first few sentences of the article. Create a summary that stands \n",
    "on its own and covers the entire article's important content.\"\"\"\n",
    "\n",
    "def create_phi_prompt(text, language=\"fr\"):\n",
    "    \"\"\"Create a language-specific prompt for summarization using Phi-4 chat format\"\"\"\n",
    "    system_message = get_system_message(language)\n",
    "    user_message = f\"Please summarize the following article: {text}\"\n",
    "    prompt = f\"<|system|>{system_message}<|end|><|user|>{user_message}<|end|><|assistant|>\"\n",
    "    return prompt\n",
    "\n",
    "def generate_summary(model, tokenizer, text, language=\"fr\", max_length=3072, max_new_tokens=512, debug=False):\n",
    "    \"\"\"Generate a summary with debugging info\"\"\"\n",
    "    # Create prompt\n",
    "    prompt = create_phi_prompt(text, language)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\nPrompt begins with: {prompt[:100]}...\")\n",
    "        print(f\"Prompt ends with: ...{prompt[-50:]}\")\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(model.device)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Input length in tokens: {len(inputs.input_ids[0])}\")\n",
    "    \n",
    "    # Generate summary\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode and extract only the summary part\n",
    "    input_length = inputs.input_ids[0].size(0)\n",
    "    new_tokens = outputs[0][input_length:]\n",
    "    summary = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Generated {len(new_tokens)} new tokens\")\n",
    "        print(f\"Raw output begins with: {summary[:100]}...\")\n",
    "    \n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generating Summaries ===\n",
      "Generating with base model...\n",
      "\n",
      "Prompt begins with: <|system|>Vous êtes un expert en résumé. Votre tâche est de créer des résumés concis et complets \n",
      "qu...\n",
      "Prompt ends with: ...]]\n",
      "[[Catégorie:Transactinide]]<|end|><|assistant|>\n",
      "Input length in tokens: 2524\n",
      "Generated 512 new tokens\n",
      "Raw output begins with: Le nihonium (Nh), élément de numéro atomique 113, a été synthétisé pour la première fois en juillet ...\n",
      "\n",
      "Generating with fine-tuned model...\n",
      "\n",
      "Prompt begins with: <|system|>Vous êtes un expert en résumé. Votre tâche est de créer des résumés concis et complets \n",
      "qu...\n",
      "Prompt ends with: ...]]\n",
      "[[Catégorie:Transactinide]]<|end|><|assistant|>\n",
      "Input length in tokens: 2524\n",
      "Generated 512 new tokens\n",
      "Raw output begins with: Le nihonium (Nh) est l'élément chimique de numéro atomique 113, synthétisé en juillet 2004 par une é...\n",
      "\n",
      "=== Comparing Summaries ===\n",
      "\n",
      "Reference summary:\n",
      "Le nihonium (symbole Nh) est un élément chimique de numéro atomique 113, initialement appelé ununtrium. Il a été synthétisé pour la première fois en juillet 2004 au RIKEN près de Tokyo, et son identification a été validée par l'IUPAC en décembre 2015. Le nihonium est un transactinide radioactif, avec l'isotope le plus stable, le 286Nh, ayant une période radioactive de 19,6 secondes. Il est situé sous le thallium dans le tableau périodique et pourrait avoir des propriétés chimiques similaires à celles d'un métal pauvre. L'IUPAC a officiellement nommé l'élément en novembre 2016 en référence au Japon. L'élément a été initialement découvert par une équipe russo-américaine en août 2003, mais la synthèse japonaise a été confirmée comme première.\n",
      "\n",
      "Base model summary:\n",
      "Le nihonium (Nh), élément de numéro atomique 113, a été synthétisé pour la première fois en juillet 2004 par une équipe de scientifiques russes et américains. Il est un élément radioactif avec une demi-vie de 19,6 secondes. Situé sous le thallium dans le tableau périodique, il est probable qu'il possède des propriétés chimiques similaires à celles d'un métal pauvre. L'IUPAC a validé son identification en décembre 2015 et lui a attribué le nom nihonium en novembre 2016, en référence au Japon. Le nihonium est l'un des éléments prévus par Mendeleïev, connu sous le nom d'éka-thallium. Les isotopes connus du nihonium incluent le {{exp|286}}Nh, avec une demi-vie de 19,6 secondes. Il est un élément transactinide très radioactif. Les détails historiques et les noms précédents du nihonium sont également mentionnés. La synthèse a été réalisée à l'Institut unifié de recherches nucléaires (JINR) et au Laboratoire national de Lawrence Livermore (LLNL) aux États-Unis, et à l'Institut de recherche sur les accélérateurs (INRA) en France. Les isotopes connus incluent le {{exp|278}}Nh et le {{exp|286}}Nh. Le nihonium est l'un des éléments prévus par Mendeleïev et est également connu sous le nom d'éka-thallium. Il est un élément radioactif avec une demi-vie de 19,6 secondes et est situé sous le thallium dans le tableau périodique. Il est probable que ses propriétés chimiques ressemblent à celles d'un métal pauvre. Il est l'un des éléments prévus par Mendeleïev et est également connu sous le nom d'éka-thallium. Il est un élément radioactif avec une demi-vie de 19,6 secondes et est situé sous le thallium dans le tableau périodique. Il est probable que ses propriétés chimiques ressemblent à celles d'un métal pauvre. Il est l'un des éléments prévus par Mendeleïev et est également connu sous le nom d'éka-thallium. Il est un élément radioactif avec une demi-vie de 19,6 secondes et est situé sous le thallium dans le tableau périodique. Il est probable que ses propriétés chimiques\n",
      "\n",
      "Fine-tuned model summary:\n",
      "Le nihonium (Nh) est l'élément chimique de numéro atomique 113, synthétisé en juillet 2004 par une équipe japonaise. Il est un élément radioactif très instable, avec l'isotope le plus stable ayant une demi-vie de 19,6 secondes. Situé sous le thallium dans le tableau périodique, il pourrait avoir des propriétés chimiques similaires à un métal pauvre. Le nihonium a été nommé en novembre 2016 en référence au Japon, pays où il a été identifié pour la première fois avec certitude. Il est également connu sous le nom d'eka-thallium, prédit par Mendeleïev. La découverte a été confirmée par l'IUPAC en décembre 2015. L'élément a été synthétisé par une réaction impliquant des isotopes de bismuth et de zinc, suivie d'une désintégration en nihonium et ensuite en éléments plus légers. La synthèse a été confirmée par l'IUPAC en décembre 2015 et le nom nihonium a été attribué en novembre 2016. L'élément est un élément radioactif très instable et son isotope le plus stable, le ^{286}Nh, a une demi-vie de 19,6 secondes. Situé sous le thallium dans le tableau périodique, il pourrait avoir des propriétés chimiques similaires à un métal pauvre. La découverte a été confirmée par l'IUPAC en décembre 2015 et le nom nihonium a été attribué en novembre 2016. L'élément est un élément radioactif très instable et son isotope le plus stable, le ^{286}Nh, a une demi-vie de 19,6 secondes. Situé sous le thallium dans le tableau périodique, il pourrait avoir des propriétés chimiques similaires à un métal pauvre. La découverte a été confirmée par l'IUPAC en décembre 2015 et le nom nihonium a été attribué en novembre 2016. L'élément est un élément radioactif très instable et son isotope le plus stable, le ^{286}Nh, a une demi-vie de 19,6 secondes. Situé sous le thallium dans le tableau périodique, il pourrait avoir des propriétés chimiques similaires à un métal pauvre. La découverte a été confirmée par l'IUPAC en décembre 2015 et le nom\n",
      "\n",
      "=== ROUGE Scores ===\n",
      "Base model:\n",
      "ROUGE-1: 0.5268\n",
      "ROUGE-2: 0.3345\n",
      "ROUGE-L: 0.5268\n",
      "\n",
      "Fine-tuned model:\n",
      "ROUGE-1: 0.6486\n",
      "ROUGE-2: 0.3920\n",
      "ROUGE-L: 0.5946\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 5. Generate summaries for analysis\n",
    "print(\"\\n=== Generating Summaries ===\")\n",
    "\n",
    "print(\"Generating with base model...\")\n",
    "base_summary = generate_summary(base_model, base_tokenizer, sample['text'], LANGUAGE, debug=True)\n",
    "\n",
    "print(\"\\nGenerating with fine-tuned model...\")\n",
    "finetuned_summary = generate_summary(finetuned_model, base_tokenizer, sample['text'], LANGUAGE, debug=True)\n",
    "\n",
    "# 6. Analyze results\n",
    "print(\"\\n=== Comparing Summaries ===\")\n",
    "print(\"\\nReference summary:\")\n",
    "print(sample['summary'])\n",
    "print(\"\\nBase model summary:\")\n",
    "print(base_summary)\n",
    "print(\"\\nFine-tuned model summary:\")\n",
    "print(finetuned_summary)\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "try:\n",
    "    rouge = Rouge()\n",
    "    base_scores = rouge.get_scores(base_summary, sample['summary'])[0]\n",
    "    ft_scores = rouge.get_scores(finetuned_summary, sample['summary'])[0]\n",
    "    \n",
    "    print(\"\\n=== ROUGE Scores ===\")\n",
    "    print(\"Base model:\")\n",
    "    print(f\"ROUGE-1: {base_scores['rouge-1']['f']:.4f}\")\n",
    "    print(f\"ROUGE-2: {base_scores['rouge-2']['f']:.4f}\")\n",
    "    print(f\"ROUGE-L: {base_scores['rouge-l']['f']:.4f}\")\n",
    "    \n",
    "    print(\"\\nFine-tuned model:\")\n",
    "    print(f\"ROUGE-1: {ft_scores['rouge-1']['f']:.4f}\")\n",
    "    print(f\"ROUGE-2: {ft_scores['rouge-2']['f']:.4f}\")\n",
    "    print(f\"ROUGE-L: {ft_scores['rouge-l']['f']:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating ROUGE scores: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Examining Training Artifacts ===\n",
      "No training_args.json found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 7. Check training artifacts\n",
    "print(\"\\n=== Examining Training Artifacts ===\")\n",
    "\n",
    "# Check for training args\n",
    "training_args_path = os.path.join(FINETUNED_MODEL, \"training_args.json\")\n",
    "if os.path.exists(training_args_path):\n",
    "    try:\n",
    "        with open(training_args_path, \"r\") as f:\n",
    "            training_args = json.load(f)\n",
    "        print(\"Found training_args.json with keys:\", list(training_args.keys()))\n",
    "        \n",
    "        # Extract important training parameters\n",
    "        if training_args:\n",
    "            print(f\"Learning rate: {training_args.get('learning_rate')}\")\n",
    "            print(f\"Batch size: {training_args.get('per_device_train_batch_size')}\")\n",
    "            print(f\"Gradient accumulation: {training_args.get('gradient_accumulation_steps')}\")\n",
    "            print(f\"Training epochs: {training_args.get('num_train_epochs')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading training args: {e}\")\n",
    "else:\n",
    "    print(\"No training_args.json found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Dataset Preprocessing ===\n",
      "Training prompt format begins with: <|system|>Vous êtes un expert en résumé. Votre tâche est de créer des résumés concis et complets \n",
      "qu...\n",
      "Training prompt format ends with: ...otiques à large spectre. Les complications peuvent inclure un choc septique, une anurie, et un coma.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 8. Verify dataset preprocessing\n",
    "print(\"\\n=== Testing Dataset Preprocessing ===\")\n",
    "\n",
    "def create_prompt_as_in_training(text, summary, language=\"fr\"):\n",
    "    \"\"\"Recreate the prompt format used during training\"\"\"\n",
    "    system_message = get_system_message(language)\n",
    "    user_message = f\"Please summarize the following article: {text}\"\n",
    "    full_prompt = f\"<|system|>{system_message}<|end|><|user|>{user_message}<|end|><|assistant|>{summary}\"\n",
    "    return full_prompt\n",
    "\n",
    "# Get a training example\n",
    "train_sample = dataset[\"train\"][0]\n",
    "training_prompt = create_prompt_as_in_training(train_sample['text'], train_sample['summary'], LANGUAGE)\n",
    "print(f\"Training prompt format begins with: {training_prompt[:100]}...\")\n",
    "print(f\"Training prompt format ends with: ...{training_prompt[-100:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Potential Issues ===\n",
      "1. Model loading: Successfully loaded model as PEFT/LoRA\n",
      "2. Prompt format: Using Phi-4 chat format with system + user message structure\n",
      "3. Generation parameters: Using temperature=0.7, top_p=0.9, max_new_tokens=512\n",
      "\n",
      "=== Recommendations ===\n",
      "1. Verify the article text format - ensure it starts from the beginning not middle of content\n",
      "2. Check training data quality - ensure summaries are proper summaries not article sections\n",
      "3. Try different checkpoint from training or retrain with more focused data\n",
      "4. Add explicit instructions in the prompt like 'Create a brief summary about this person:'\n",
      "5. Try truncating article text to just first few paragraphs for clearer summarization\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 9. Issues and recommendations\n",
    "print(\"\\n=== Potential Issues ===\")\n",
    "print(\"1. Model loading: Successfully loaded model as\", \"PEFT/LoRA\" if is_peft_model else \"standard model\")\n",
    "print(\"2. Prompt format: Using Phi-4 chat format with system + user message structure\")\n",
    "print(\"3. Generation parameters: Using temperature=0.7, top_p=0.9, max_new_tokens=512\")\n",
    "\n",
    "# Check output quality\n",
    "if \"ession\" in base_summary[:50] or \"ession\" in finetuned_summary[:50]:\n",
    "    print(\"4. Output quality issue: Both models may be continuing from middle of article text\")\n",
    "    print(\"   - This suggests the model might be receiving text from the middle of the article\")\n",
    "    print(\"   - Or the model is not properly trained to summarize from the beginning\")\n",
    "\n",
    "# Generate recommendations\n",
    "print(\"\\n=== Recommendations ===\")\n",
    "print(\"1. Verify the article text format - ensure it starts from the beginning not middle of content\")\n",
    "print(\"2. Check training data quality - ensure summaries are proper summaries not article sections\")\n",
    "print(\"3. Try different checkpoint from training or retrain with more focused data\")\n",
    "print(\"4. Add explicit instructions in the prompt like 'Create a brief summary about this person:'\")\n",
    "print(\"5. Try truncating article text to just first few paragraphs for clearer summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
